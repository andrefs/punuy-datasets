{
  "id": "multisimlex",
  "metadata": {
    "name": "Multi-SimLex",
    "authors": "Ivan VuliÄ‡, Simon Baker, Edoardo Maria Ponti, Ulla Petti, Ira Leviant, Kelly Wing, Olga Majewska, Eden Bar, Matt Malone, Thierry Poibeau, Roi Reichart and Anna Korhonen",
    "urls": ["https://multisimlex.com/"],
    "papers": [
      {
        "title": "Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity",
        "url": "https://multisimlex.com/Multi-SimLex-Paper.pdf"
      }
    ],
    "date": "2020",
    "description": "Multi-SimLex is a large-scale multilingual resource for lexical semantics. The current version of Multi-SimLex provides human judgments on the semantic similarity of word pairs for as many as 12 monolingual and 66 cross-lingual datasets. The languages covered are typologically diverse and represent both major languages (e.g., Mandarin Chinese, Spanish, Russian) and less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels.",
    "downloadUrls": ["https://multisimlex.com/#download"],
    "languages": ["en"],
    "domain": "general",
    "relationTypes": ["similarity"]
  },
  "partitions": [
    {
      "id": "ENG",
      "relationType": "similarity",
      "scale": {
        "value": {
          "max": 6,
          "min": 0
        }
      },
      "metrics": {
        "annotators": {
          "total": 13,
          "minEachPair": 13
        },
        "interAnnoAgreement": [
          {
            "method": "spearman",
            "value": 0.698,
            "metric": "APIAA"
          },
          {
            "method": "spearman",
            "value": 0.794,
            "metric": "AMIAA"
          }
        ]
      }
    }
  ],
  "originalInstructions": "All annotators must be required to abide by the following instructions:\n\nEach annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity.\n\nEach annotator must score the entire set of 1,888 pairs in the dataset. The pairs must not be shared between different annotators.\n\nAnnotators are able to break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.\n\nAnnotators are kept anonymous, and are not able to communicate with each other during the annotation process."
}
